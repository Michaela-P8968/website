---
title: "Project 2"
author: "Michaela Priess"
date: "11/26/2019"
output: html_document
---



<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 3.2.1     ✔ purrr   0.3.3
## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.4.0</code></pre>
<pre><code>## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(dplyr)
library(ggplot2)
library(lmtest)</code></pre>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<pre class="r"><code>library(sandwich)
library(plotROC)
library(glmnet)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Attaching package: &#39;Matrix&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:tidyr&#39;:
## 
##     expand, pack, unpack</code></pre>
<pre><code>## Loaded glmnet 3.0</code></pre>
<pre class="r"><code>library(carData)</code></pre>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The Mroz data set provides information about women’s labor-force participation in the United States. The data was collected from the Panel Study of Income Dynamics (PSID), and all of the observations were made for married women. This data set was obtained from R. Overall, the Mroz data set contains 8 variables. The “lfp” variable indicates whether a woman participates or doesn’t participate in the labor-force. The “k5” variable indicates the number of children 5 years of age or younger a woman has. The “k618” variable indicates the number of children 6-18 years old that a woman has. The “age” variable indicates how old the woman is in years. The “wc” variable indicates if the woman attended college. The “hc” variable indicates if the woman’s husband attended college. The “lwg” variable shows the woman’s wage rate if they are in the labor-force. If the are not in the labor-force, a value was given based on a regression the study did of lwg on the other variables. The “inc” variable indicates family income not including the wife’s income.</p>
</div>
<div id="part-1" class="section level2">
<h2>Part 1</h2>
<p>In total, I performed 1 MANOVA and 4 ANOVAs for a total of 5 tests. Since the categorical response variable only has two levels, it was not necessary to perform post-hoc t-tests. I did not include the numeric variable “lwg” in the MANOVA because I didn’t think it made sense to add it considering a log-wage rate was calculated for women not participating in the labor-force.With a total of 5 tests, there is a 0.226 probability of making at least one type I error. Using bonferroni correction, the significance level should be adjusted to 0.01. With this adjusted significance level, only two variables, “k5” and “inc”, showed significant differences in labor force participation. Before the adjustment, age was a significant variable, but it lost significance once the adjustment was made. In both instances, the k618 variable was not significant. For a MANOVA, there are many assumptions that need to be met such as random samples, multivariate normality of the dependent variables, homogeneity of within group covariance matrices, linear relationships between the dependent variables, no extreme mutlivariate outliers, and no multicollinearity. It is very likely that all of the assumptions were not met with this data. The sample is not completely random since only married women were included in the study, and it is likely that not every dependent variable meets multivariate normality.</p>
<pre class="r"><code>#MANOVA
man &lt;- manova(cbind(k5, k618, age, inc) ~ lfp, data = Mroz)
summary(man)</code></pre>
<pre><code>##            Df   Pillai approx F num Df den Df    Pr(&gt;F)    
## lfp         1 0.094913    19.61      4    748 2.315e-15 ***
## Residuals 751                                              
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>#ANOVAs
summary.aov(man)</code></pre>
<pre><code>##  Response k5 :
##              Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## lfp           1   9.432  9.4324  35.955 3.136e-09 ***
## Residuals   751 197.016  0.2623                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response k618 :
##              Df  Sum Sq Mean Sq F value Pr(&gt;F)
## lfp           1    0.01  0.0077  0.0044  0.947
## Residuals   751 1310.03  1.7444               
## 
##  Response age :
##              Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## lfp           1    318  317.55  4.8982 0.02718 *
## Residuals   751  48688   64.83                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##  Response inc :
##              Df Sum Sq Mean Sq F value   Pr(&gt;F)   
## lfp           1   1408 1407.76  10.531 0.001226 **
## Residuals   751 100389  133.67                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>#Probability of at least one type I error
1 - ((1 - 0.05)^5)</code></pre>
<pre><code>## [1] 0.2262191</code></pre>
<pre class="r"><code>#Bonferroni correction
0.05/5</code></pre>
<pre><code>## [1] 0.01</code></pre>
</div>
<div id="part-2" class="section level2">
<h2>Part 2</h2>
<p>I chose to run a randomized t-test to see if there is a significant mean difference in log wage rate between women who attended college and women who did not go to college. The actual mean difference in log wage rate between the college and no college group was 0.406. For the randomization test, the null hypothesis was that the mean log wage rate is the same for women who attended college and women who did not attend college. The alternative hypothesis was that the mean log wage rate is different for women who attended college versus women who did not attend college. The p-value of the randomization test was 0. This indicates that there is a significant mean difference in log wage rate between women who attended college and women who did not attend college. It would be very unlikely to get a mean difference as large as the actual mean difference when looking at a randomly sampled distribution.</p>
<pre class="r"><code>#H0: The mean log wage rate is the same for women who attended college vs. did not attend college.
#HA:The mean log wage rate is different for women who attended college vs. did not attend college.

#Actual Mean Difference
Mroz %&gt;% group_by(wc) %&gt;% summarise(means = mean(lwg)) %&gt;% summarise(diff(means))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   `diff(means)`
##           &lt;dbl&gt;
## 1         0.406</code></pre>
<pre class="r"><code>#Randomization Test
random_dist &lt;- vector()

for (i in 1:5000){
  new &lt;- data.frame(lwg = sample(Mroz$lwg), wc = Mroz$wc)
  random_dist[i] &lt;- mean(new[new$wc == &quot;yes&quot;,]$lwg) - 
    mean(new[new$wc == &quot;no&quot;,]$lwg)
}

#p-value
mean(random_dist &gt; 0.406)*2</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>#Plot
hist(random_dist, main = &quot;Null Distribution&quot;, xlab = &quot;Mean Differences&quot;, xlim = c(-0.2, 0.5)); abline(v = 0.406, col = &quot;red&quot;)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="part-3" class="section level2">
<h2>Part 3</h2>
<p>Controlling for age, there was a significant difference in log wage rate between women who attended college and women who did not attend college. When controlling for age, women who did attend college had a wage rate that was on average 0.406 greater than that of women who did not attend college. When controlling for college attendance, there was not a significant effect of age on log wage rate. When controlling for college attendance, for every 1 year increase in age, log wage rate only increased by 0.00392 on average. The effect of age on log wage rate was 0.00538 less when a woman did attend college as compared to a woman that did not attend college. However, this interaction was not significant.</p>
<pre class="r"><code>#Mean-center age
age_c &lt;- Mroz$age - mean(Mroz$age, na.rm = TRUE)

#Linear Regression
lrmod &lt;- lm(lwg ~ wc * age_c, data = Mroz)
summary(lrmod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lwg ~ wc * age_c, data = Mroz)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.15219 -0.24077 -0.00177  0.27414  2.26307 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.981448   0.024045  40.817   &lt;2e-16 ***
## wcyes        0.406423   0.045452   8.942   &lt;2e-16 ***
## age_c        0.003922   0.003009   1.303    0.193    
## wcyes:age_c -0.005375   0.005557  -0.967    0.334    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5588 on 749 degrees of freedom
## Multiple R-squared:  0.09904,    Adjusted R-squared:  0.09543 
## F-statistic: 27.44 on 3 and 749 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>#Regression Plot 
ggplot(Mroz, aes(x = age_c, y = lwg, color = wc)) + 
  geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, fullrange = T) +
  xlab(&quot;Age (years)&quot;) + ylab(&quot;Log Wage Rate&quot;) +
  labs(color = &quot;Attended College&quot;) + ggtitle(&quot;Effects of Age and College Attendance on Log Wage Rate&quot;)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Not all of the assumptions were met for a linear regression. For the most part, the linearity assumption seemed to be met. According to the Kolmogorov-Smirnov test, the assumption of normality was not met. According to the Breusch-Pagan test, the homoskedasticity assumption was met.</p>
<pre class="r"><code>#Checking Assumptions
resids &lt;- lrmod$residuals
fitvals &lt;- lrmod$fitted.values

#Linearity Assumption 
ggplot() + geom_point(aes(x = fitvals, y = resids)) + geom_hline(yintercept = 0, color = &quot;red&quot;)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>#Normality Assumption 
ks.test(resids, &quot;pnorm&quot;, mean=0, sd(resids))</code></pre>
<pre><code>## Warning in ks.test(resids, &quot;pnorm&quot;, mean = 0, sd(resids)): ties should not be
## present for the Kolmogorov-Smirnov test</code></pre>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  resids
## D = 0.09457, p-value = 2.828e-06
## alternative hypothesis: two-sided</code></pre>
<pre class="r"><code>#Homoskedasticity Assumption
bptest(lrmod)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  lrmod
## BP = 5.8957, df = 3, p-value = 0.1168</code></pre>
<p>When the regression was rerun using robust standard errors, the results did not differ from the first linear regression that was run. Attending college still had a significant effect on log wage rate. Like in the first linear regression, age did not have a significant effect on log wage rate, and the interaction between attending college and age was still not significant. Using the adjusted R-squared, this model only acounts for 0.0954 of the variation in the log wage rate (the outcome).</p>
<pre class="r"><code>#Regression with robust standard errors
coeftest(lrmod, vcov = vcovHC(lrmod))</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##               Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept)  0.9814482  0.0222551 44.0999 &lt; 2.2e-16 ***
## wcyes        0.4064227  0.0506856  8.0185 4.115e-15 ***
## age_c        0.0039215  0.0028711  1.3659    0.1724    
## wcyes:age_c -0.0053750  0.0058269 -0.9224    0.3566    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>#Using adjusted R-squared, my model accounts for a proportion of 0.0954 of the variation in the outcome.</code></pre>
</div>
<div id="part-4" class="section level2">
<h2>Part 4</h2>
<p>The bootstrapped standard errors were not the exact same as those seen in the original linear regression, but they were all extremely similar to the original standard errors obtained. The bootstrapped standard errors were similar to the robust standard errors as well, but they weren’t as similar to them as they were to the original standard errors. Due to this similarity in standard errors, the p-values are probably quite similar as well.</p>
<pre class="r"><code>#Bootstrap Residuals
resid_resamp &lt;- replicate(5000,{
  new_resids &lt;- sample(resids, replace = TRUE)
  newdat &lt;- Mroz
  newdat$new_y &lt;- fitvals + new_resids
  fit2 &lt;- lm(new_y ~ wc * age_c, data = newdat)
  coef(fit2)
})

#Bootstrapped Standard Errors
resid_resamp %&gt;% t %&gt;% as.data.frame %&gt;% summarize_all(sd)</code></pre>
<pre><code>##   (Intercept)      wcyes       age_c wcyes:age_c
## 1  0.02375552 0.04530931 0.002953455 0.005497751</code></pre>
</div>
<div id="part-5" class="section level2">
<h2>Part 5</h2>
<p>Both having kids 5 and under and being older in age significantly decreases the log-odds of participating in the labor-force. Controlling for age, for every one unit increase in number of kids 5 and under, the odds of participating in the labor force is multiplied by a factor of 0.267. Controlling for number of kids 5 and under, for every one year increase in age, the odds of participating in the labor force is multiplied by a factor of 0.943.</p>
<pre class="r"><code>#Logistic Regression
logmod &lt;- glm(lfp ~ k5 + age, data = Mroz, family = &quot;binomial&quot;)
coeftest(logmod)</code></pre>
<pre><code>## 
## z test of coefficients:
## 
##              Estimate Std. Error z value  Pr(&gt;|z|)    
## (Intercept)  3.085783   0.497118  6.2073 5.389e-10 ***
## k5          -1.320421   0.186377 -7.0847 1.394e-12 ***
## age         -0.058467   0.010900 -5.3642 8.132e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>exp(coef(logmod))</code></pre>
<pre><code>## (Intercept)          k5         age 
##  21.8845911   0.2670230   0.9432093</code></pre>
<pre class="r"><code>#Confusion Matrix
prob &lt;- predict(logmod, type = &quot;response&quot;)
table(prediction = as.numeric(prob &gt; 0.5), truth = Mroz$lfp) %&gt;% addmargins</code></pre>
<pre><code>##           truth
## prediction  no yes Sum
##        0   139  88 227
##        1   186 340 526
##        Sum 325 428 753</code></pre>
<p>This model had an accuracy of 0.636, a sensitivity of 0.794, a specificity of 0.428, and precision of 0.646. Overall, these numbers weren’t great. This model did fairly well in regards to sensitivity, in that it correctly identified 79.4% of the women who do participate in the labor force. The model did poorly in regards to specificity in that it only correctly identified 42.8% of the women who do not participate in the labor force. The model also did poorly in regards to precision in that only 64.6% of women predicted to be participating in the labor force actually were. The accuracy of the model wasn’t great either in that only 63.6% of the total test population was correctly identified as either participating or not participating in the labor force.</p>
<pre class="r"><code>#Accuracy = 0.636
(340 + 139)/753</code></pre>
<pre><code>## [1] 0.6361222</code></pre>
<pre class="r"><code>#Sensitivity (TPR) = 0.794
340/428</code></pre>
<pre><code>## [1] 0.7943925</code></pre>
<pre class="r"><code>#Specificity (TNR) = 0.428
139/325</code></pre>
<pre><code>## [1] 0.4276923</code></pre>
<pre class="r"><code>#Precision (PPV) = 0.646
340/526</code></pre>
<pre><code>## [1] 0.6463878</code></pre>
<pre class="r"><code>#Density Plot
logit &lt;- predict(logmod)

ggplot(Mroz, aes(logit, fill = Mroz$lfp)) + geom_density(alpha = 0.3) +
  geom_vline(xintercept = 0, lty = 2) + xlab(&quot;Log-Odds&quot;) +
  ylab(&quot;Density&quot;) + ggtitle(&quot;Density of Log-Odds by Labor-Force Participation&quot;) +
  labs(fill = &quot;Labor-Force Participation&quot;)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The shape of the ROC curve itslef indicates that this model does not have good predictive capabilities, and the AUC further reinforces this. The calculated AUC for this model is 0.668 which is poor.</p>
<pre class="r"><code>#ROC Curve and AUC Calculation
Mroz1 &lt;- Mroz %&gt;% mutate(y = case_when(lfp == &quot;yes&quot; ~ 1, lfp == &quot;no&quot; ~ 0))

ROC_plot &lt;- ggplot(Mroz1) + geom_roc(aes(d = y, m = prob), n.cuts = 0)
ROC_plot</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>calc_auc(ROC_plot)</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.6677175</code></pre>
<p>After performing a 10-fold cross validation, the accuracy, sensitivity, and recall values were very similar to those that were originally obtained. When first run, accuracy was 0.620, sensitivity was 0.796, specificity was 0.398, precision was 0.634, and the auc was 0.670.</p>
<pre class="r"><code>#10-fold CV

class_diag&lt;-function(probs,truth){
  
  tab&lt;-table(factor(probs&gt;.5,levels=c(&quot;FALSE&quot;,&quot;TRUE&quot;)),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE &amp; is.logical(truth)==FALSE) truth&lt;-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord&lt;-order(probs, decreasing=TRUE)
  probs &lt;- probs[ord]; truth &lt;- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup&lt;-c(probs[-1]&gt;=probs[-length(probs)], FALSE)
  TPR&lt;-c(0,TPR[!dup],1); FPR&lt;-c(0,FPR[!dup],1)
  
  n &lt;- length(TPR)
  auc&lt;- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}

set.seed(1234)
k = 10
data &lt;- Mroz[sample(nrow(Mroz)),]
folds &lt;- cut(seq(1:nrow(Mroz)),breaks = k, labels = FALSE)

diags &lt;- NULL
for (i in 1:k){
  train &lt;- data[folds != i,]
  test &lt;- data[folds == i,]
  truth &lt;- test$lfp
  
  fit &lt;- glm(lfp ~ k5 + age, data = train, family = &quot;binomial&quot;)
  probs &lt;- predict(fit, newdata = test, type = &quot;response&quot;)
  
  diags &lt;- rbind(diags, class_diag(probs, truth))
}

apply(diags, 2, mean)</code></pre>
<pre><code>##       acc      sens      spec       ppv       auc 
## 0.6321228 0.7844033 0.4302078 0.6462114 0.6610212</code></pre>
</div>
<div id="part-6" class="section level2">
<h2>Part 6</h2>
<p>The retained variables, or the variables with a non-zero coefficient, were k5 (kids 5 and under), age, wcyes (wife attended college), lwg (log wage rate), and inc (family income excluding the wife’s income).</p>
<pre class="r"><code>#LASSO Regression
Mroz_fit &lt;- glm(lfp ~., data = Mroz, family = &quot;binomial&quot;)

response_matrix &lt;- as.matrix(Mroz$lfp)
predictor_matrix &lt;- model.matrix(Mroz_fit)[,-1] 

cv &lt;- cv.glmnet(predictor_matrix, response_matrix, family = &quot;binomial&quot;)
lasso &lt;- glmnet(predictor_matrix, response_matrix, family = &quot;binomial&quot;, lambda = cv$lambda.1se)

coefficients(lasso)</code></pre>
<pre><code>## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                      s0
## (Intercept)  1.42467904
## k5          -0.84007569
## k618         .         
## age         -0.02739342
## wcyes        0.43149326
## hcyes        .         
## lwg          0.38331321
## inc         -0.01557889</code></pre>
<p>This model crafted based on the results of the LASSO regression did only slightly better in terms of out of sample accuracy than the model in part 5 did. The out of sample accuracy for this model was 0.680, and the out of sample accuracy for the logistic regression model in part 5 was 0.620. This doesn’t show a significant improvement in the predictive ability between the two models.</p>
<pre class="r"><code>#10-fold CV
sig_vars &lt;- model.matrix(Mroz_fit)[,c(&quot;k5&quot;, &quot;age&quot;, &quot;wcyes&quot;, &quot;lwg&quot;, &quot;inc&quot;)]
sigvars_dat &lt;- as.data.frame(sig_vars)
sigvars_dat$lfp &lt;- Mroz$lfp

set.seed(1234)
k = 10

data2 &lt;- sigvars_dat[sample(nrow(sigvars_dat)),]
folds2 &lt;- cut(seq(1:nrow(sigvars_dat)), breaks = k, labels = FALSE)

diags &lt;- NULL
for (i in 1:k){
  train &lt;- data2[folds != i,]
  test &lt;- data2[folds == i,]
  truth &lt;- test$lfp
  
  fit &lt;- glm(lfp ~., data = train, family = &quot;binomial&quot;)
  probs &lt;- predict(fit, newdata = test, type = &quot;response&quot;)
  
  diags &lt;- rbind(diags, class_diag(probs, truth))
}

apply(diags, 2, mean)</code></pre>
<pre><code>##       acc      sens      spec       ppv       auc 
## 0.6785965 0.7754679 0.5487775 0.6959681 0.7211265</code></pre>
</div>
